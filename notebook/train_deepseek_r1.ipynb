{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Import PyTorch and Hugging Face Transformers\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Import dataset utilities\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import libraries from TRL (Transformers Reinforcement Learning)\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead, \n",
    "    PPOConfig, \n",
    "    PPOTrainer, \n",
    "    GRPOTrainer, \n",
    "    GRPOConfig, \n",
    "    SFTTrainer\n",
    ")\n",
    "\n",
    "# Import math-related utilities\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       " 'solution': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       " 'messages': [{'content': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the \"AI-MO/NuminaMath-TIR\" dataset from DigitalLearningGmbH\n",
    "MATH_le = load_dataset(\"AI-MO/NuminaMath-TIR\", \"default\")  \n",
    "\n",
    "# Access the first sample in the training set\n",
    "MATH_le['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Bespoke-Stratos contains 17K problems focused on math and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 151665\n",
      "Model max length: 131072\n",
      "Pad token: <|endoftext|>\n",
      "EOS token: <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"../checkpoint/grpo/qwen_0.5\" # For saving our trained model\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer with chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some basic info about the model, take a look at the total number of parameters our base model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# Initialize base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close to 0.5B params, let’s print a simple response from it and then we will move on to next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Test Input: how are you?\n",
      "Model Response: system\n",
      "You are Qwen, a helpful assistant.\n",
      "user\n",
      "how are you?\n",
      "assistant\n",
      "Hello! I'm currently functioning perfectly and am not experiencing any physical sensations or emotions at the moment. I don't have feelings, so there's nothing for me to say about my well-being. Is there anything specific you'd like to know or talk about that I can help with? I'm here to provide information and answer questions as best as I can based on my programming. If you're interested in learning more, feel free to ask, and I'll do my best to assist you!\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Test basic inference\n",
    "def test_model_inference(user_input: str):\n",
    "    \"\"\"Test basic model inference with the loaded model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the output of this tiny model is quite reliable and suitable for our DeepSeek lookalike model training for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek system prompt for GRPO based training\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to structure the training data\n",
    "def make_conversation(example):\n",
    "    \"\"\"Convert dataset examples into conversation format.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take each problem column value from our training dataset and return a dictionary with the system prompt and the appended problem question for each row. Let’s create this function that will prepare our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "def load_math_dataset():\n",
    "    \"\"\"Load and prepare the mathematics dataset.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"AI-MO/NuminaMath-TIR\",\n",
    "        name=\"default\",\n",
    "        split=['train', 'test']\n",
    "    )\n",
    "    \n",
    "    # Convert splits into dictionary\n",
    "    dataset = {\n",
    "        'train': dataset[0].select(range(10000)),\n",
    "        'test': dataset[1]\n",
    "    }\n",
    "    \n",
    "    # Apply conversation format\n",
    "    for split in dataset:\n",
    "        dataset[split] = dataset[split].map(make_conversation)\n",
    "\n",
    "        # Remove 'messages' column if exists\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything ready, let’s transform our training data into the required format and print the training and test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 10000\n",
      "Test set size: 99\n"
     ]
    }
   ],
   "source": [
    "# Load our training dataset and printing train/test size\n",
    "dataset = load_math_dataset()\n",
    "\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our training dataset, we need to validate our dataset (**Check if user/assistant conversation exist**) before moving to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating train split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n",
      "\n",
      "Validating test split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(dataset):\n",
    "    \"\"\"Perform basic validation checks on the dataset.\"\"\"\n",
    "    \n",
    "    # Define the required fields for the dataset\n",
    "    required_fields = [\"problem\", \"prompt\"]\n",
    "\n",
    "    # Loop through the 'train' and 'test' splits of the dataset\n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"\\nValidating {split} split:\")\n",
    "\n",
    "        # Retrieve column names from the dataset\n",
    "        fields = dataset[split].column_names\n",
    "\n",
    "        # Check if any required fields are missing\n",
    "        missing = [field for field in required_fields if field not in fields]\n",
    "        if missing:\n",
    "            print(f\"Warning: Missing fields: {missing}\")  # Warn if fields are missing\n",
    "        else:\n",
    "            print(\"✓ All required fields present\")  # Confirm all fields are present\n",
    "\n",
    "        # Retrieve the first sample from the dataset split\n",
    "        sample = dataset[split][0]\n",
    "\n",
    "        # Extract the 'prompt' field, which contains a list of messages\n",
    "        messages = sample['prompt']\n",
    "\n",
    "        # Validate the prompt format:\n",
    "        # - It should contain at least two messages\n",
    "        # - The first message should be from the 'system' role\n",
    "        # - The second message should be from the 'user' role\n",
    "        if (len(messages) >= 2 and\n",
    "            messages[0]['role'] == 'system' and\n",
    "            messages[1]['role'] == 'user'):\n",
    "            print(\"✓ Prompt format is correct\")  # Confirm correct format\n",
    "        else:\n",
    "            print(\"Warning: Incorrect prompt format\")  # Warn if format is incorrect\n",
    "\n",
    "# Validate dataset\n",
    "validate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training dataset is validated successfully 🙌, it means we have successfully transformed our dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Functions\n",
    "\n",
    "We already saw in GRPO section that it evaluate the answer of base model through five different ways:\n",
    "\n",
    " 1. **Accuracy** (is the answer correct?)\n",
    "\n",
    " 2. **Format** (are the `<think>` and `<answer>` tags used properly?)\n",
    "\n",
    " 3. **Reasoning Steps** (is the logic clear?)\n",
    "\n",
    " 4. **Cosine Scaling** (is the response concise?)\n",
    "\n",
    " 5. **Repetition Penalty** (is there unnecessary repetition?).\n",
    "\n",
    "Each of these are functions will calculate the reward for each response, and we need to code them. So, let’s do that first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Reward\n",
    "\n",
    "Accuracy reward is the most easy to understand but requires a bit complex code. In this reward model we want to check if mathematically our base model response is equivalent to the ground truth solution.\n",
    "\n",
    "If the model answer is mathematically correct, we assign a reward of **1.0**. If it is incorrect, the reward is **0.0**. In cases where the ground truth solution cannot be parsed, we assign a neutral reward of **0.5** to avoid unfair penalties.\n",
    "\n",
    "Now, let’s implement the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the model's response is mathematically \n",
    "    equivalent to the ground truth solution.\n",
    "    Uses latex2sympy2 for parsing and math_verify for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract responses\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "    \n",
    "    for content, sol in zip(contents, solutions):\n",
    "        # Parse the ground truth solution\n",
    "        gold_parsed = parse(sol, extraction_mode=\"first_match\", \n",
    "                            extraction_config=[LatexExtractionConfig()])\n",
    "        \n",
    "        if gold_parsed:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "#                             equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            # Reward 1.0 if correct, 0.0 if incorrect\n",
    "            reward = float(verify(answer_parsed, gold_parsed))\n",
    "        else:\n",
    "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
    "            reward = 0.5\n",
    "            print(\"Warning: Failed to parse gold solution:\", sol)\n",
    "\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we check whether the model response is **equivalent** to the correct answer. Instead of comparing raw text, we:\n",
    "\n",
    " 1. Convert the solution into a structured mathematical format using **latex2sympy2**.\n",
    "\n",
    " 2. If parsing fails, assign a neutral reward of **0.5**.\n",
    "\n",
    " 3. Extract the model output and normalize it for better robustness.\n",
    "\n",
    " 4. Use **math_verify** to check if the parsed response matches the parsed solution.\n",
    "\n",
    " 5. If correct assign **1,** if incorrect assign **0**.\n",
    "\n",
    "This ensures that accuracy evaluation is not just about textual similarity but **true mathematical correctness.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Reward\n",
    "\n",
    "Format Reward is all about making sure our model follows instructions and structures its output correctly. We asked it to put its reasoning in `<think>` tags and the final answer in `<answer>` tags, right? This reward function checks exactly that!\n",
    "\n",
    "If the model uses those tags correctly, we give it a reward of 1. If it messes up the format, it gets 0. Simple as that! This encourages the model to pay attention to the output structure we want.\n",
    "\n",
    "Let’s code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Format Reward Function\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the completion has the correct format:\n",
    "    <think>...</think> <answer>...</answer>.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for the desired format\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "\n",
    "    # Extract the content from each completion\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Check if each completion matches the pattern\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward 1.0 for correct format, 0.0 otherwise\n",
    "    return [1.0 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function:\n",
    "\n",
    "* We define a pattern using regular expressions (regex). This pattern basically says “the content should *start* with <think>, have *anything* inside until </think>, then some *spaces*, then <answer>, *anything* inside until </answer>, and then *end* there”.\n",
    "\n",
    "* We get the actual text content from each model completion.\n",
    "\n",
    "* Then we use use re.match to see if each content perfectly matches our pattern. re.DOTALL helps the . in regex match newlines too, and re.MULTILINE makes ^ and $ match the start/end of the whole string, not just lines.\n",
    "\n",
    "* Finally, we give a reward 1 if it matched the format perfectly, 0 if it didn’t. This is a strict on/off reward for format correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning Steps Reward\n",
    "\n",
    "Reasoning Steps Reward is a bit clever. We want to encourage our model to show its **“thinking process”**. So, we are going to reward it for including things that *look like* reasoning steps.\n",
    "\n",
    "We will look for keywords and patterns that usually show up in step-by-step reasoning, like:\n",
    "\n",
    "* Step 1, Step 2, etc.\n",
    "\n",
    "* Numbered lists like 1, 2\n",
    "\n",
    "* Bullet points like - or *\n",
    "\n",
    "* Transition words like First, Second, Next, Finally\n",
    "\n",
    "The more of these it includes, the better the reward. It’s like giving points for showing its work!\n",
    "\n",
    "Let’s code this reasoning encouraging function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    r\"\"\"\n",
    "    Reward function to encourage clear step-by-step reasoning.\n",
    "    It looks for patterns like \"Step 1:\", numbered lists, bullet points,\n",
    "    and transition words.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find indicators of reasoning steps\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "\n",
    "    # Extract completion contents\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Count the number of reasoning step indicators in each completion\n",
    "    matches = [len(re.findall(pattern, content, re.MULTILINE))\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward is proportional to the number of reasoning steps, maxing out at 1.0\n",
    "    # We're using a \"magic number\" 3 here - encourage at least 3 steps for full reward\n",
    "    return [min(1.0, count / 3) for count in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create a pattern that’s a bit more complex regex. It looks for all those reasoning indicator things we listed above.\n",
    "\n",
    "We use re.findall to find *all* the matches of our pattern within each content. `len(re.findall(…))` then gives us the *count* of these indicators.\n",
    "\n",
    "The reward is calculated as min(1.0, count / 3). This means\n",
    "\n",
    "* If it finds 3 or more reasoning indicators ( count >= 3), the reward is 1.0 (max reward).\n",
    "\n",
    "* If it finds fewer (e.g., count = 1 or 2), it gets a *partial* reward (like 1/3 or 2/3).\n",
    "\n",
    "* If it finds none (count = 0), the reward is 0.0.\n",
    "\n",
    "The / 3 is a bit of a magic number here. We’re saying **“aim for about 3 reasoning steps to get full credit”** You can tweak this number if you want to encourage more or fewer steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Scaled Reward\n",
    "\n",
    "Cosine Scaled Reward is a bit more advanced. It’s about encouraging *conciseness* in correct answers and being *less harsh* on longer incorrect answers.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **For correct answers:** We want to reward *shorter*, more direct solutions more than long, rambling ones. A short, correct answer is often better.\n",
    "\n",
    "* **For incorrect answers:** A short, wrong answer is probably worse than a longer, wrong answer that at least *tried* to reason. So, we want to penalize short wrong answers *more* than long wrong answers.\n",
    "\n",
    "Let’s see the code that does this clever scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Cosine Scaled Reward Function\n",
    "def get_cosine_scaled_reward(\n",
    "    min_value_wrong: float = -0.5,\n",
    "    max_value_wrong: float = -0.1,\n",
    "    min_value_correct: float = 0.8,\n",
    "    max_value_correct: float = 1.0,\n",
    "    max_len: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a cosine scaled reward function. This function scales the accuracy reward\n",
    "    based on completion length. Shorter correct solutions get higher rewards,\n",
    "    longer incorrect solutions get less penalty.\n",
    "    \"\"\"\n",
    "    def cosine_scaled_reward(completions, solution, accuracy_rewards, **kwargs):\n",
    "        \"\"\"\n",
    "        Cosine scaled reward function that adjusts accuracy rewards based on completion length.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "\n",
    "        for content, sol, acc_reward in zip(contents, solution, accuracy_rewards):\n",
    "            gen_len = len(content)  # Length of the generated answer\n",
    "            progress = gen_len / max_len # How far we are to max length\n",
    "            cosine = math.cos(progress * math.pi) # Cosine value based on progress\n",
    "\n",
    "            if acc_reward > 0.5: # Assuming accuracy_reward gives ~1.0 for correct answers\n",
    "                min_value = min_value_correct\n",
    "                max_value = max_value_correct\n",
    "            else: # Incorrect answer\n",
    "                min_value = max_value_wrong  # Note the swap!\n",
    "                max_value = min_value_wrong\n",
    "\n",
    "            # Cosine scaling formula!\n",
    "            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
    "            rewards.append(float(reward))\n",
    "        return rewards\n",
    "    return cosine_scaled_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_cosine_scaled_reward(...)` generates a reward function for training, customizing scaling with parameters like min_value_wrong/max_value_wrong (penalty range for incorrect answers) and min_value_correct/max_value_correct (reward range for correct ones). max_len sets the maximum length for scaling.\n",
    "\n",
    "Inside, `cosine_scaled_reward(...)` we calculate rewards based on completions, solution, and accuracy_rewards.\n",
    "\n",
    "It computes gen_len, normalizes it as progress `= gen_len / max_len`, and derives a cosine value that starts at 1 (short answers) and decreases to -1 (long answers).\n",
    "\n",
    "If `acc_reward > 0.5`, it uses the correct reward range, otherwise it applies the incorrect range but swaps min/max values to penalize longer wrong answers less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition Penalty Reward\n",
    "\n",
    "Repetition Penalty Reward is all about discouraging our model from getting stuck in loops and repeating itself. We want it to generate fresh, varied reasoning and answers, not just copy-paste the same phrases over and over!\n",
    "\n",
    "This reward function penalizes the model if it uses the same sequences of words (n-grams) too many times. We’ll use n-grams of size 3 (trigrams) in our example, but you can adjust this.\n",
    "\n",
    "If the model repeats itself a lot, it gets a negative reward (penalty). If it’s more diverse and avoids repetition, the penalty is less.\n",
    "\n",
    "Let’s implement the code to penalize repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetition_penalty_reward(ngram_size: int = 3, max_penalty: float = -0.1):\n",
    "    \"\"\"\n",
    "    Returns a repetition penalty reward function. Penalizes repetitions of n-grams\n",
    "    in the generated text.\n",
    "    \"\"\"\n",
    "    if max_penalty > 0:\n",
    "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
    "\n",
    "    def zipngram(text: str, ngram_size: int):\n",
    "        \"\"\"Helper function to generate n-grams from text.\"\"\"\n",
    "        words = text.lower().split() # Lowercase and split into words\n",
    "        return zip(*[words[i:] for i in range(ngram_size)]) # Create n-grams\n",
    "\n",
    "    def repetition_penalty_reward(completions, **kwargs) -> float:\n",
    "        \"\"\"\n",
    "        Repetition penalty reward function.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "        for completion in contents:\n",
    "            if completion == \"\": # No penalty for empty completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            if len(completion.split()) < ngram_size: # No penalty for short completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            ngrams = set() # Use a set to store unique n-grams\n",
    "            total = 0\n",
    "            for ng in zipngram(completion, ngram_size): # Generate n-grams\n",
    "                ngrams.add(ng) # Add n-gram to the set (duplicates are ignored)\n",
    "                total += 1 # Count total n-grams\n",
    "\n",
    "            # Calculate scaling factor: more repetition -> higher scaling\n",
    "            scaling = 1 - len(ngrams) / total\n",
    "            reward = scaling * max_penalty # Apply penalty based on scaling\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    return repetition_penalty_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `get_repetition_penalty_reward(...)` creates a reward function to penalize repetition, with parameters like ngram_size (default 3, for trigrams) and max_penalty (a negative value, e.g., -0.1).\n",
    "\n",
    "A helper function, `zipngram(text, ngram_size)`, generates n-grams by converting text to lowercase, splitting it into words, and using `zip(*[words[i:] for i in range(ngram_size)])` for efficient extraction.\n",
    "\n",
    "Inside, `repetition_penalty_reward(...)` computes the penalty for each completion. If it's empty or too short, it gets a reward of 0.0.\n",
    "\n",
    "The penalty scales as scaling `= 1 - len(ngrams) / total`, where total is the number of n-grams and len(ngrams) is the unique count. More repetition makes scaling approach 1, increasing the penalty.\n",
    "\n",
    "The final reward is scaling * max_penalty, meaning less repetition results in a smaller penalty, while high repetition leads to a stronger negative reward. \n",
    "\n",
    ">We have implemented all five reward functions, Let’s move on to next stage where we define our training args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configurations for R1 Zero\n",
    "\n",
    "Now we to code a configuration where we can fine-tune how our *reward functions* actually work. So, Let’s define that configuration class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRPOScriptArguments for reward function parameters\n",
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
    "        },\n",
    "    )\n",
    "    cosine_min_value_wrong: float = field(\n",
    "        default=-0.5,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_max_value_wrong: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_min_value_correct: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_value_correct: float = field(\n",
    "        default=1.0,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_len: int = field(\n",
    "        default=1000,\n",
    "        metadata={\"help\": \"Maximum length for cosine scaling\"},\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `@dataclass` decorator makes it easy to create a class for storing data. WhileGRPOScriptArguments class holds reward settings.\n",
    "\n",
    "The reward_funcs list decides which rewards to use, starting with [\"accuracy\", \"format\"], but you can add more like \"reasoning_steps\", \"cosine\", \"repetition_penalty\".\n",
    "\n",
    "Some settings control how the cosine_scaled_reward and repetition_penalty_reward work, letting you adjust how rewards are given.\n",
    "\n",
    "Next up, we have TrainingArguments from the transformers library. This is the **main** configuration object that controls almost **everything** about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TrainingArguments from transformers\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,          # Output directory for checkpoints and logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # Total number of training epochs\n",
    "    per_device_train_batch_size=32,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n",
    "    learning_rate=5e-5,            # Initial learning rate for AdamW optimizer\n",
    "    warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
    "    weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
    "    logging_steps=10,              # Log every X updates steps\n",
    "    eval_strategy=\"steps\",    # Evaluate every `eval_steps`\n",
    "    eval_steps=1000,                 # Evaluation and logging steps\n",
    "#     eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",         # Save checkpoint every `save_steps`\n",
    "    save_steps=50,                 # Save checkpoint every X updates steps\n",
    "    save_total_limit=2,            # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    dataloader_num_workers=2,      # Number of subprocesses to use for data loading\n",
    "    seed=42,                       # Random seed for reproducibility\n",
    "    bf16=True,                     # Use mixed precision BFP16 training\n",
    "    push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
    "#     gradient_checkpointing=True,   # Enable gradient checkpointing\n",
    "    report_to=\"none\",              # Reporting to no one\n",
    "    remove_unused_columns=False,   # Do not remove unused columns from the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to have a ModelConfig. This is where we put settings that are specific to the **model itself**, like which pre-trained model to use, what data type to use (like bfloat16), and whether to trust remote code or not and so.\n",
    "\n",
    "Let’s define our ModelConfig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=MODEL_NAME, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"bfloat16\", metadata={\"help\": \"Override the default `torch_dtype` and load the model under this dtype.\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True, metadata={\"help\": \"Trust remote code when loading model and tokenizer.\"}\n",
    "    )\n",
    "    attn_implementation: Optional[str] = field(\n",
    "        default=\"flash_attention_2\", metadata={\"help\": \"Attention implementation to use. 'flash_attention_2' or None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **ModelConfig** class holds key settings, including model_name_or_path, which defaults to **Qwen 0.5B Instruct**. We use torch_dtype=\"bfloat16\" for efficiency and set trust_remote_code=True for safe remote loading. Additionally, attn_implementation=\"flash_attention_2\" is enabled for potentially faster training if supported.\n",
    "\n",
    "Now we need to actually **create** instances of these configuration classes so we can use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate configuration objects\n",
    "script_args = GRPOScriptArguments()\n",
    "model_args = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get our list of reward functions and any “callbacks” we want to use during training.\n",
    "\n",
    "Callbacks are like little helpers that can do things at different points in the training process (like logging progress, saving models, etc.). For now, we’ll just use a simple logging callback.\n",
    "\n",
    "Getting our reward functions in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get reward functions based on script arguments\n",
    "def get_reward_functions(script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "    reward_funcs_registry = {\n",
    "        \"accuracy\": accuracy_reward,  # Assuming accuracy_reward is defined in previous steps\n",
    "        \"format\": format_reward,      # Assuming format_reward is defined in previous steps\n",
    "        \"reasoning_steps\": reasoning_steps_reward, # Assuming reasoning_steps_reward is defined\n",
    "        \"cosine\": get_cosine_scaled_reward( # Assuming get_cosine_scaled_reward is defined\n",
    "            min_value_wrong=script_args.cosine_min_value_wrong,\n",
    "            max_value_wrong=script_args.cosine_max_value_wrong,\n",
    "            min_value_correct=script_args.cosine_min_value_correct,\n",
    "            max_value_correct=script_args.cosine_max_value_correct,\n",
    "            max_len=script_args.cosine_max_len,\n",
    "        ),\n",
    "        \"repetition_penalty\": get_repetition_penalty_reward( # Assuming get_repetition_penalty_reward is defined\n",
    "            ngram_size=script_args.repetition_n_grams,\n",
    "            max_penalty=script_args.repetition_max_penalty,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our callback function which will track loss and other important info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A simple callback for logging training information at specific steps.\n",
    "    \"\"\"\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            if state.log_history and len(state.log_history) > 0:\n",
    "                logger.info(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('loss', None)}, Learning Rate = {state.log_history[-1].get('learning_rate', None)}\")\n",
    "            else:\n",
    "                logger.info(f\"Step {state.global_step}: No logging information available yet\")\n",
    "\n",
    "def get_callbacks(training_args, model_args, script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks to be used during training.\n",
    "    For now, it includes only the LoggingCallback. You can extend this to add more callbacks.\n",
    "    \"\"\"\n",
    "    callbacks = [LoggingCallback()] # Instantiate our LoggingCallback\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, initializing these function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reward functions and callbacks\n",
    "reward_functions = get_reward_functions(script_args)\n",
    "callbacks = get_callbacks(training_args, model_args, script_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Loop\n",
    "\n",
    "This is the engine that will actually drive our GRPO training. We need to initialize it, giving it all the pieces we’ve prepared: our model, reward functions, training arguments, dataset, and callbacks!\n",
    "\n",
    "Let’s initialize the GRPOTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create GRPOConfig from TrainingArguments\n",
    "grpo_config = GRPOConfig(\n",
    "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
    "    **{ \n",
    "       # REMOVED model_init_kwargs here \n",
    "       # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
    "    }\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,                      # Our initialized Qwen model\n",
    "    reward_funcs=reward_functions,    # List of reward functions from previous step\n",
    "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
    "    train_dataset=dataset['train'],   # Training dataset\n",
    "    eval_dataset=dataset['test'],    # Evaluation dataset\n",
    "    callbacks=callbacks              # List of callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the **Training Loop**! This is as simple as calling the train() method on our grpo_trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 1:41:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Start the GRPO Training Loop\n",
    "train_result = grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this cell, you should see the training process begin.\n",
    "\n",
    "Training will take some time but we set **num_train_epochs = 1** and are using a small model, it shouldn’t take *too* long for this example.\n",
    "\n",
    "But for real-world GRPO DeepSeek R1 Zero training, you’d likely train for many more epochs and steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Tiny R1 Zero LLM\n",
    "\n",
    "Once the training completed, we can save our trained model which can be used for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your trained model (same as OUTPUT_DIR)\n",
    "TRAINED_MODEL_PATH = \"../checkpoint/grpo/qwen_0.5\"\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "# Save the trained model\n",
    "grpo_trainer.save_model(TRAINED_MODEL_PATH)\n",
    "\n",
    "print(f\"GRPO Trained model saved to {TRAINED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply load the trained model using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer - make sure to use trust_remote_code=True if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model config requires it\n",
    "    padding_side=\"left\" # Ensure consistent padding side\n",
    ")\n",
    "\n",
    "# Set pad token if it wasn't saved or loaded correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the trained model itself\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model architecture requires it\n",
    "    torch_dtype=torch.bfloat16 # Keep the same dtype as training for consistency\n",
    ")\n",
    "\n",
    "# Move the loaded model to your device (GPU if available)\n",
    "trained_model.to(device) # 'device' is still our CUDA device from before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Inference with the Trained Model\n",
    "def test_trained_model_inference(user_input: str):\n",
    "    \"\"\"Test inference with the loaded trained model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, # Re-use our system prompt\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template using our tokenizer\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output using our *trained_model*\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200, # Maybe generate a bit longer now\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens back to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_trained_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Trained Model Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
